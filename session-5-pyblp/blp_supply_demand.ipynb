{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Coefficients Logit Tutorial with the BLP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyblp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pyblp.options.digits = 2\n",
    "pyblp.options.verbose = False\n",
    "pyblp.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll use data from [Berry, Levinsohn, and Pakes (1995)](https://pyblp.readthedocs.io/en/latest/references.html#berry-levinsohn-and-pakes-1995) to solve the paper's automobile problem. This tutorial exhibits some other features of pyblp:\n",
    "\n",
    "- Incorporating a supply side into demand estimation.\n",
    "- Allowing for simple price-income demographic effects.\n",
    "- Calculating clustered standard errors.\n",
    "\n",
    "\n",
    "## Theory of Random Coefficients Logit\n",
    "\n",
    "The random coefficients model extends the plain logit model by allowing for correlated tastes for different product characteristics.\n",
    "In this  model (indirect) utility is given by\n",
    "\n",
    "$$u_{ijt} = \\alpha_i p_{jt} + x_{jt} \\beta_i^\\text{ex} + \\xi_{jt} + \\epsilon_{ijt}$$\n",
    "\n",
    "The main addition is that $\\beta_i = (\\alpha_i, \\beta_i^\\text{ex})$ have individual specific subscripts $i$.\n",
    "\n",
    "Conditional on $\\beta_i$, the individual market share follow the same logit form as before. But now we must integrate over heterogeneous individuals to get the aggregate market share:\n",
    "\n",
    "$$s_{jt}(\\alpha, \\beta, \\theta) = \\int \\frac{\\exp(\\alpha_i p_{jt} + x_{jt} \\beta_i^\\text{ex} + \\xi_{jt})}{1 + \\sum_k \\exp(\\alpha_i p_{jt} + x_{kt} \\beta_i^\\text{ex} + \\xi_{kt})} f(\\alpha_i, \\beta_i \\mid \\theta).$$\n",
    "\n",
    "In general, this integral needs to be calculated numerically. This also means that we can't directly linearize the model. It is common to re-parametrize the model to separate the aspects of mean utility that all individuals agree on, $\\delta_{jt} = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt}$, from the individual specific heterogeneity, $\\mu_{ijt}(\\theta)$. This gives us\n",
    "\n",
    "$$s_{jt}(\\delta_{jt}, \\theta) = \\int \\frac{\\exp(\\delta_{jt} + \\mu_{ijt})}{1 + \\sum_k \\exp(\\delta_{kt} + \\mu_{ikt})} f(\\mu_{it} | \\theta).$$\n",
    "\n",
    "Given a guess of $\\theta$ we can solve the system of nonlinear equations for the vector $\\delta$ which equates observed and predicted market share $s_{jt} = s_{jt}(\\delta, \\theta)$. Now we can perform a linear IV GMM regression of the form\n",
    "\n",
    "$$\\delta_{jt}(\\theta) = \\alpha p_{jt} + x_{jt} \\beta^\\text{ex} + \\xi_{jt}.$$\n",
    "\n",
    "The moments are constructed by interacting the predicted residuals $\\hat{\\xi}_{jt}(\\theta)$ with instruments $z_{jt}$ to form\n",
    "\n",
    "$$\\bar{g}(\\theta) =\\frac{1}{N} \\sum_{j,t} z_{jt}' \\hat{\\xi}_{jt}(\\theta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Coefficients\n",
    "\n",
    "To include random coefficients we need to add a [`Formulation`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Formulation.html#pyblp.Formulation) for the demand-side nonlinear characteristics $X_2$.\n",
    "\n",
    "Just like in the logit case we have the same reserved field names in `product_data`:\n",
    "\n",
    "- `market_ids` are the unique market identifiers which we subscript $t$.\n",
    "- `shares` specifies the market share which need to be between zero and one, and within a market ID, $\\sum_{j} s_{jt} < 1$.\n",
    "- `prices` are prices $p_{jt}$. These have some special properties and are _always_ treated as endogenous.\n",
    "- `demand_instruments0`, `demand_instruments1`, and so on are numbered demand instruments. These represent only the _excluded_ instruments. The exogenous regressors in $X_1$ (of which $X_2$ is typically a subset) will be automatically added to the set of instruments.\n",
    "\n",
    "We proceed with the following steps:\n",
    "\n",
    "1. Load the `product data` which at a minimum consists of `market_ids`, `shares`, `prices`, and at least a single column of demand instruments, `demand_instruments0`.\n",
    "2. Define a [`Formulation`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Formulation.html#pyblp.Formulation) for the $X_1$ (linear) demand model.\n",
    "\n",
    "    - This and all other formulas are similar to R and [patsy](https://patsy.readthedocs.io/en/stable/) formulas.\n",
    "    - It includes a constant by default. To exclude the constant, specify either a `0` or a `-1`.\n",
    "    - To efficiently include fixed effects, use the `absorb` option and specify which categorical variables you would like to absorb.\n",
    "    - Some model reduction may happen automatically. The constant will be excluded if you include fixed effects and some precautions are taken against collinearity. However, you will have to make sure that differently-named variables are not collinear.\n",
    "\n",
    "3. Define a [`Formulation`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Formulation.html#pyblp.Formulation) for the $X_2$ (nonlinear) demand model.\n",
    "\n",
    "    - Include only the variables over which we want random coefficients.\n",
    "    - Do not absorb or include fixed effects.\n",
    "    - It will include a random coefficient on the constant (to capture inside good vs. outside good preference) unless you specify not to with a `0` or a `-1`.\n",
    "\n",
    "4. Define an [`Integration`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Integration.html#pyblp.Integration) configuration to solve the market share integral from several available options:\n",
    "\n",
    "    - Monte Carlo integration (pseudo-random draws).\n",
    "    - Product rule quadrature.\n",
    "    - Sparse grid quadrature.\n",
    "\n",
    "3. Combine [`Formulation`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Formulation.html#pyblp.Formulation) classes, `product_data`, and the [`Integration`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Integration.html#pyblp.Integration) configuration to construct a [`Problem`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Problem.html#pyblp.Problem).\n",
    "4. Use the [`Problem.solve`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Problem.solve.html#pyblp.Problem.solve) method to estimate paramters.\n",
    "\n",
    "    - It is required to specify an initial guess of the nonlinear parameters. This serves two primary purposes: speeding up estimation and indicating to the solver through initial values of zero which parameters are restricted to be always zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of Random Taste Parameters\n",
    "\n",
    "It is common to assume that $f(\\beta_i \\mid \\theta)$ follows a multivariate normal distribution, and to break it up into three parts:\n",
    "\n",
    "1. A mean $K_1 \\times 1$ taste which all individuals agree on, $\\beta$.\n",
    "2. A $K_2 \\times K_2$ covariance matrix, $V$. As is common with multivariate normal distributions, $V$ is not estimated directly. Rather, its matrix square (Cholesky) root $\\Sigma$ is estimated where $\\Sigma\\Sigma' = V$.\n",
    "3. Any $K_2 \\times D$ interactions, $\\Pi$, with observed $D \\times 1$ demographic data, $d_i$.\n",
    "\n",
    "Together this gives us that\n",
    "\n",
    "$$\\beta_i \\sim N(\\beta + \\Pi d_i, \\Sigma\\Sigma').$$\n",
    "\n",
    "[`Problem.solve`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Problem.solve.html#pyblp.Problem.solve) takes an initial guess $\\Sigma_0$ of $\\Sigma$. It guarantees that $\\hat{\\Sigma}$ (the estimated parameters) will have the same sparsity structure as $\\Sigma_0$. So any zero element of $\\Sigma$ is restricted to be zero in the solution $\\hat{\\Sigma}$. For example, a popular restriction is that $\\Sigma$ is diagonal, this can be achieved by passing a diagonal matrix as $\\Sigma_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "We'll use [pandas](https://pandas.pydata.org/) to load two sets of data:\n",
    "\n",
    "1. `product_data`, which contains prices, shares, and other product characteristics.\n",
    "2. `agent_data`, which contains draws from the distribution of heterogeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_ids</th>\n",
       "      <th>clustering_ids</th>\n",
       "      <th>car_ids</th>\n",
       "      <th>firm_ids</th>\n",
       "      <th>region</th>\n",
       "      <th>shares</th>\n",
       "      <th>prices</th>\n",
       "      <th>hpwt</th>\n",
       "      <th>air</th>\n",
       "      <th>mpd</th>\n",
       "      <th>...</th>\n",
       "      <th>supply_instruments2</th>\n",
       "      <th>supply_instruments3</th>\n",
       "      <th>supply_instruments4</th>\n",
       "      <th>supply_instruments5</th>\n",
       "      <th>supply_instruments6</th>\n",
       "      <th>supply_instruments7</th>\n",
       "      <th>supply_instruments8</th>\n",
       "      <th>supply_instruments9</th>\n",
       "      <th>supply_instruments10</th>\n",
       "      <th>supply_instruments11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1971</td>\n",
       "      <td>AMGREM71</td>\n",
       "      <td>129</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>4.935802</td>\n",
       "      <td>0.528997</td>\n",
       "      <td>0</td>\n",
       "      <td>1.888146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.705933</td>\n",
       "      <td>1.595656</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-61.959985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.060389</td>\n",
       "      <td>29.786989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.888146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1971</td>\n",
       "      <td>AMHORN71</td>\n",
       "      <td>130</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>5.516049</td>\n",
       "      <td>0.494324</td>\n",
       "      <td>0</td>\n",
       "      <td>1.935989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.680910</td>\n",
       "      <td>1.490295</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-61.959985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.060389</td>\n",
       "      <td>29.786989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.935989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1971</td>\n",
       "      <td>AMJAVL71</td>\n",
       "      <td>132</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>7.108642</td>\n",
       "      <td>0.467613</td>\n",
       "      <td>0</td>\n",
       "      <td>1.716799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.801067</td>\n",
       "      <td>1.357703</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-61.959985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.060389</td>\n",
       "      <td>29.786989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.716799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1971</td>\n",
       "      <td>AMMATA71</td>\n",
       "      <td>134</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>6.839506</td>\n",
       "      <td>0.426540</td>\n",
       "      <td>0</td>\n",
       "      <td>1.687871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.818061</td>\n",
       "      <td>1.261347</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-61.959985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.060389</td>\n",
       "      <td>29.786989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.687871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1971</td>\n",
       "      <td>AMAMBS71</td>\n",
       "      <td>136</td>\n",
       "      <td>15</td>\n",
       "      <td>US</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>8.928395</td>\n",
       "      <td>0.452489</td>\n",
       "      <td>0</td>\n",
       "      <td>1.504286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.933210</td>\n",
       "      <td>1.237365</td>\n",
       "      <td>87.0</td>\n",
       "      <td>-61.959985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.060389</td>\n",
       "      <td>29.786989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.504286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   market_ids clustering_ids  car_ids  firm_ids region    shares    prices  \\\n",
       "0        1971       AMGREM71      129        15     US  0.001051  4.935802   \n",
       "1        1971       AMHORN71      130        15     US  0.000670  5.516049   \n",
       "2        1971       AMJAVL71      132        15     US  0.000341  7.108642   \n",
       "3        1971       AMMATA71      134        15     US  0.000522  6.839506   \n",
       "4        1971       AMAMBS71      136        15     US  0.000442  8.928395   \n",
       "\n",
       "       hpwt  air       mpd  ...  supply_instruments2  supply_instruments3  \\\n",
       "0  0.528997    0  1.888146  ...                  0.0             1.705933   \n",
       "1  0.494324    0  1.935989  ...                  0.0             1.680910   \n",
       "2  0.467613    0  1.716799  ...                  0.0             1.801067   \n",
       "3  0.426540    0  1.687871  ...                  0.0             1.818061   \n",
       "4  0.452489    0  1.504286  ...                  0.0             1.933210   \n",
       "\n",
       "   supply_instruments4  supply_instruments5  supply_instruments6  \\\n",
       "0             1.595656                 87.0           -61.959985   \n",
       "1             1.490295                 87.0           -61.959985   \n",
       "2             1.357703                 87.0           -61.959985   \n",
       "3             1.261347                 87.0           -61.959985   \n",
       "4             1.237365                 87.0           -61.959985   \n",
       "\n",
       "   supply_instruments7  supply_instruments8  supply_instruments9  \\\n",
       "0                  0.0            46.060389            29.786989   \n",
       "1                  0.0            46.060389            29.786989   \n",
       "2                  0.0            46.060389            29.786989   \n",
       "3                  0.0            46.060389            29.786989   \n",
       "4                  0.0            46.060389            29.786989   \n",
       "\n",
       "   supply_instruments10  supply_instruments11  \n",
       "0                   0.0              1.888146  \n",
       "1                   0.0              1.935989  \n",
       "2                   0.0              1.716799  \n",
       "3                   0.0              1.687871  \n",
       "4                   0.0              1.504286  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_data = pd.read_csv(pyblp.data.BLP_PRODUCTS_LOCATION)\n",
    "product_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `product_data` contains market IDs, product IDs, firm IDs, shares, prices, a number of product characteristics, and instruments. The product IDs are called `clustering_ids` because they will be used to compute clustered standard errors. For more information about the instruments and the example data as a whole, refer to the [`data`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.data.html#module-pyblp.data) module.\n",
    "\n",
    "The `agent_data` contains market IDs, integration weights $w_{it}$, integration nodes $\\nu_{it}$, and demographics $d_{it}$. Here we use the $I_t = 200$ importance sampling weights and nodes from the original paper.\n",
    "\n",
    "In non-example problems, it is usually a better idea to use many more draws, or a more sophisticated [`Integration`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Integration.html#pyblp.Integration) configuration such as sparse grid quadrature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_ids</th>\n",
       "      <th>weights</th>\n",
       "      <th>nodes0</th>\n",
       "      <th>nodes1</th>\n",
       "      <th>nodes2</th>\n",
       "      <th>nodes3</th>\n",
       "      <th>nodes4</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1971</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.192188</td>\n",
       "      <td>0.478777</td>\n",
       "      <td>0.980830</td>\n",
       "      <td>-0.824410</td>\n",
       "      <td>2.473301</td>\n",
       "      <td>109.560369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1971</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>1.497074</td>\n",
       "      <td>-2.026204</td>\n",
       "      <td>-1.741316</td>\n",
       "      <td>1.412568</td>\n",
       "      <td>-0.747468</td>\n",
       "      <td>45.457314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1971</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>1.438081</td>\n",
       "      <td>0.813280</td>\n",
       "      <td>-1.749974</td>\n",
       "      <td>-1.203509</td>\n",
       "      <td>0.049558</td>\n",
       "      <td>127.146548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1971</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>1.768655</td>\n",
       "      <td>-0.177453</td>\n",
       "      <td>0.286602</td>\n",
       "      <td>0.391517</td>\n",
       "      <td>0.683669</td>\n",
       "      <td>22.604045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1971</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.849970</td>\n",
       "      <td>-0.135337</td>\n",
       "      <td>0.735920</td>\n",
       "      <td>1.036247</td>\n",
       "      <td>-1.143436</td>\n",
       "      <td>170.226032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   market_ids   weights    nodes0    nodes1    nodes2    nodes3    nodes4  \\\n",
       "0        1971  0.000543  1.192188  0.478777  0.980830 -0.824410  2.473301   \n",
       "1        1971  0.000723  1.497074 -2.026204 -1.741316  1.412568 -0.747468   \n",
       "2        1971  0.000544  1.438081  0.813280 -1.749974 -1.203509  0.049558   \n",
       "3        1971  0.000701  1.768655 -0.177453  0.286602  0.391517  0.683669   \n",
       "4        1971  0.000549  0.849970 -0.135337  0.735920  1.036247 -1.143436   \n",
       "\n",
       "       income  \n",
       "0  109.560369  \n",
       "1   45.457314  \n",
       "2  127.146548  \n",
       "3   22.604045  \n",
       "4  170.226032  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_data = pd.read_csv(pyblp.data.BLP_AGENTS_LOCATION)\n",
    "agent_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Problem\n",
    "\n",
    "Unlike the fake cereal problem, we won't absorb any fixed effects in the automobile problem, so the linear part of demand $X_1$ has more components. We also need to specify a formula for the random coefficients $X_2$, including a random coefficient on the constant, which captures correlation among all inside goods.\n",
    "\n",
    "The primary new addition to the model relative to the fake cereal problem is that we add a supply side formula for product characteristics that contribute to marginal costs, $X_3$. The [patsy](https://patsy.readthedocs.io/en/stable/)-style formulas support functions of regressors such as the `log` function used below.\n",
    "\n",
    "We stack the three product formulations in order: $X_1$, $X_2$, and $X_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1 + hpwt + air + mpd + space,\n",
       " 1 + prices + hpwt + air + mpd + space,\n",
       " 1 + log(hpwt) + air + log(mpg) + log(space) + trend)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_formulations = (\n",
    "   pyblp.Formulation('1 + hpwt + air + mpd + space'),\n",
    "   pyblp.Formulation('1 + prices + hpwt + air + mpd + space'),\n",
    "   pyblp.Formulation('1 + log(hpwt) + air + log(mpg) + log(space) + trend')\n",
    ")\n",
    "product_formulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original specification for the automobile problem includes the term $\\log(y_i - p_j)$, in which $y$ is income and $p$ are prices. Instead of including this term, which gives rise to a host of numerical problems, we'll follow [Berry, Levinsohn, and Pakes (1999)](https://pyblp.readthedocs.io/en/latest/references.html#berry-levinsohn-and-pakes-1999) and use its first-order linear approximation, $p_j / y_i$. \n",
    "\n",
    "The agent formulation for demographics, $d$, includes a column of $1 / y_i$ values, which we'll interact with $p_j$. To do this, we will treat draws of $y_i$ as demographic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I(1 / income)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_formulation = pyblp.Formulation('0 + I(1 / income)')\n",
    "#agent_formulation = pyblp.Formulation('0 + I(1 / income) + price + horsepower + fuel_efficiency + acceleration')\n",
    "\n",
    "agent_formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the cereal example, the [`Problem`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Problem.html#pyblp.Problem) can be constructed by combining the `product_formulations`, `product_data`, `agent_formulation`, and `agent_data`. We'll also choose the functional form of marginal costs $c_{jt}$. A linear marginal cost specification is the default setting, so we'll need to use the `costs_type` argument of [`Problem`](https://pyblp.readthedocs.io/en/latest/_api/pyblp.Problem.html#pyblp.Problem) to employ the log-linear specification used by [Berry, Levinsohn, and Pakes (1995)](https://pyblp.readthedocs.io/en/latest/references.html#berry-levinsohn-and-pakes-1995).\n",
    "\n",
    "When initializing the problem, we get a warning about integration weights not summing to one. This is because the above product data were created by the original paper with importance sampling. To disable this warning, we could increase `pyblp.options.weights_tol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Integration weights in the following markets sum to a value that differs from 1 by more than options.weights_tol: all markets. Sometimes this is fine, for example when weights were built with importance sampling. Otherwise, it is a sign that there is a data problem.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dimensions:\n",
       "=======================================================\n",
       " T    N     F    I     K1    K2    K3    D    MD    MS \n",
       "---  ----  ---  ----  ----  ----  ----  ---  ----  ----\n",
       "20   2217  26   4000   5     6     6     1    13    18 \n",
       "=======================================================\n",
       "\n",
       "Formulations:\n",
       "=====================================================================================\n",
       "       Column Indices:            0          1       2       3          4         5  \n",
       "-----------------------------  --------  ---------  ----  --------  ----------  -----\n",
       " X1: Linear Characteristics       1        hpwt     air     mpd       space          \n",
       "X2: Nonlinear Characteristics     1       prices    hpwt    air        mpd      space\n",
       "X3: Log Cost Characteristics      1      log(hpwt)  air   log(mpg)  log(space)  trend\n",
       "       d: Demographics         1/income                                              \n",
       "====================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem = pyblp.Problem(product_formulations, product_data, agent_formulation, agent_data, costs_type='log')\n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem outputs a table of dimensions:\n",
    "\n",
    "- $T$ denotes the number of markets.\n",
    "- $N$ is the length of the dataset (the number of products across all markets).\n",
    "- $F$ denotes the number of firms.\n",
    "- $I = \\sum_t I_t$ is the total number of agents across all markets (200 draws per market times 20 markets).\n",
    "- $K_1$ is the number of linear demand characteristics.\n",
    "- $K_2$ is the number of nonlinear demand characteristics.\n",
    "- $K_3$ is the number of linear supply characteristics.\n",
    "- $D$ is the number of demographic variables.\n",
    "- $M_D$ is the number of demand instruments, including exogenous regressors.\n",
    "- $M_S$ is the number of supply instruments, including exogenous regressors.\n",
    "\n",
    "The formulations table describes all four formulas for demand-side linear characteristics, demand-side nonlinear characteristics, supply-side characteristics, and demographics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the Problem\n",
    "\n",
    "The only remaining decisions are:\n",
    "\n",
    "- Choosing $\\Sigma$ and $\\Pi$ starting values, $\\Sigma_0$ and $\\Pi_0$.\n",
    "- Potentially choosing bounds for $\\Sigma$ and $\\Pi$.\n",
    "\n",
    "The decisions we will use are:\n",
    "\n",
    "- Use published estimates as our starting values in $\\Sigma_0$.\n",
    "- Interact the inverse of income, $1 / y_i$, only with prices, and use the published estimate on $\\log(y_i - p_j)$ as our starting value for $\\alpha$ in $\\Pi_0$.\n",
    "- Bound $\\Sigma_0$ to be positive since it is a diagonal matrix where the diagonal consists of standard deviations.\n",
    "\n",
    "When using a routine that supports bounds, it's usually a good idea to set your own more bounds so that the routine doesn't try out large parameter values that create numerical issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_sigma = np.diag([3.612, 0, 4.628, 1.818, 1.050, 2.056])\n",
    "initial_pi = np.c_[[0, -43.501, 0, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are only 5 nonzeros on the diagonal of $\\Sigma$, which means that we only need 5 columns of integration nodes to integrate over these 5 dimensions of unobserved heterogeneity. Indeed, `agent_data` contains exactly 5 columns of nodes. If we were to ignore the $\\log(y_i - p_j)$ term (by not configuring $\\Pi$) and include a term on prices in $\\Sigma$ instead, we would have needed 6 columns of integration nodes in our `agent_data`.\n",
    "\n",
    "A downside of the log-linear marginal costs specification is that nonpositive estimated marginal costs can create problems for the optimization routine when computing $\\log c(\\hat{\\theta})$. We'll use the `costs_bounds` argument to bound marginal costs from below by a small number. \n",
    "\n",
    "Finally, as in the original paper, we'll use `W_type` and `se_type` to cluster by product IDs, which were specified as `clustering_ids` in `product_data`, and set `initial_update=True` to update the initial GMM weighting matrix and the mean utility at the starting parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Problem Results Summary:\n",
       "=======================================================================================================================\n",
       "GMM   Objective    Projected    Reduced Hessian  Reduced Hessian  Clipped  Clipped  Weighting Matrix  Covariance Matrix\n",
       "Step    Value    Gradient Norm  Min Eigenvalue   Max Eigenvalue   Shares    Costs   Condition Number  Condition Number \n",
       "----  ---------  -------------  ---------------  ---------------  -------  -------  ----------------  -----------------\n",
       " 2    +5.0E+02     +8.6E-06        +4.9E-01         +5.1E+02         0        0         +4.2E+09          +3.8E+08     \n",
       "=======================================================================================================================\n",
       "\n",
       "Cumulative Statistics:\n",
       "===========================================================================\n",
       "Computation  Optimizer  Optimization   Objective   Fixed Point  Contraction\n",
       "   Time      Converged   Iterations   Evaluations  Iterations   Evaluations\n",
       "-----------  ---------  ------------  -----------  -----------  -----------\n",
       " 00:01:54       Yes          57           135         39202       120390   \n",
       "===========================================================================\n",
       "\n",
       "Nonlinear Coefficient Estimates (Robust SEs Adjusted for 999 Clusters in Parentheses):\n",
       "===================================================================================================\n",
       "Sigma:      1        prices      hpwt        air         mpd        space     |   Pi:     1/income \n",
       "------  ----------  --------  ----------  ----------  ----------  ----------  |  ------  ----------\n",
       "  1      +2.0E+00                                                             |    1      +0.0E+00 \n",
       "        (+6.1E+00)                                                            |                    \n",
       "                                                                              |                    \n",
       "prices   +0.0E+00   +0.0E+00                                                  |  prices   -4.5E+01 \n",
       "                                                                              |          (+9.2E+00)\n",
       "                                                                              |                    \n",
       " hpwt    +0.0E+00   +0.0E+00   +6.1E+00                                       |   hpwt    +0.0E+00 \n",
       "                              (+2.2E+00)                                      |                    \n",
       "                                                                              |                    \n",
       " air     +0.0E+00   +0.0E+00   +0.0E+00    +4.0E+00                           |   air     +0.0E+00 \n",
       "                                          (+2.1E+00)                          |                    \n",
       "                                                                              |                    \n",
       " mpd     +0.0E+00   +0.0E+00   +0.0E+00    +0.0E+00    +2.5E-01               |   mpd     +0.0E+00 \n",
       "                                                      (+5.5E-01)              |                    \n",
       "                                                                              |                    \n",
       "space    +0.0E+00   +0.0E+00   +0.0E+00    +0.0E+00    +0.0E+00    +1.9E+00   |  space    +0.0E+00 \n",
       "                                                                  (+1.1E+00)  |                    \n",
       "===================================================================================================\n",
       "\n",
       "Beta Estimates (Robust SEs Adjusted for 999 Clusters in Parentheses):\n",
       "==========================================================\n",
       "    1          hpwt        air         mpd        space   \n",
       "----------  ----------  ----------  ----------  ----------\n",
       " -7.3E+00    +3.5E+00    -1.0E+00    +4.2E-01    +4.2E+00 \n",
       "(+2.8E+00)  (+1.4E+00)  (+2.1E+00)  (+2.5E-01)  (+6.6E-01)\n",
       "==========================================================\n",
       "\n",
       "Gamma Estimates (Robust SEs Adjusted for 999 Clusters in Parentheses):\n",
       "======================================================================\n",
       "    1       log(hpwt)      air       log(mpg)   log(space)    trend   \n",
       "----------  ----------  ----------  ----------  ----------  ----------\n",
       " +2.8E+00    +9.0E-01    +4.2E-01    -5.2E-01    -2.6E-01    +2.7E-02 \n",
       "(+1.2E-01)  (+7.2E-02)  (+8.7E-02)  (+7.3E-02)  (+2.1E-01)  (+3.1E-03)\n",
       "======================================================================"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = problem.solve(\n",
    "    initial_sigma,\n",
    "    initial_pi,\n",
    "    costs_bounds=(0.001, None),\n",
    "    W_type='clustered',\n",
    "    se_type='clustered',\n",
    "    initial_update=True,\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some discrepancies between our results and the original paper, but results are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
